
EPOCH 1:
  batch 1 loss: 0.027628790587186813
  batch 1 loss: 5.5257581174373624e-05
  batch 2 loss: 0.02540551871061325
  batch 3 loss: 0.025205818936228752
  batch 4 loss: 0.02695707231760025
  batch 5 loss: 0.02497483231127262
  batch 6 loss: 0.023777352645993233
  batch 7 loss: 0.026984840631484985
  batch 8 loss: 0.02425747737288475
  batch 9 loss: 0.02414606884121895
  batch 10 loss: 0.025304172188043594
  batch 11 loss: 0.027700237929821014
  batch 12 loss: 0.02727380022406578
  batch 13 loss: 0.02543565072119236
  batch 14 loss: 0.02617012895643711
  batch 15 loss: 0.02781825140118599
  batch 16 loss: 0.02459745481610298
  batch 17 loss: 0.025320347398519516
  batch 18 loss: 0.022297866642475128
  batch 19 loss: 0.024190165102481842
  batch 20 loss: 0.022876519709825516
  batch 21 loss: 0.022780701518058777
  batch 22 loss: 0.02590794488787651
  batch 23 loss: 0.025432802736759186
  batch 24 loss: 0.027638010680675507
  batch 25 loss: 0.023360174149274826
  batch 26 loss: 0.026089439168572426
  batch 27 loss: 0.025116536766290665
  batch 28 loss: 0.022256139665842056
  batch 29 loss: 0.02338620461523533
  batch 30 loss: 0.023119544610381126
  batch 31 loss: 0.02348111942410469
  batch 32 loss: 0.024716299027204514
  batch 33 loss: 0.023773249238729477
  batch 34 loss: 0.02534807100892067
  batch 35 loss: 0.024926722049713135
  batch 36 loss: 0.023923445492982864
  batch 37 loss: 0.025188345462083817
  batch 38 loss: 0.024472126737236977
  batch 39 loss: 0.023487688973546028
  batch 40 loss: 0.024132736027240753
  batch 41 loss: 0.026335401460528374
  batch 42 loss: 0.02713092975318432
  batch 43 loss: 0.023367661982774734
  batch 44 loss: 0.027017850428819656
  batch 45 loss: 0.02498973347246647
  batch 46 loss: 0.02295142039656639
  batch 47 loss: 0.022901620715856552
  batch 48 loss: 0.022947018966078758
  batch 49 loss: 0.021707914769649506
  batch 50 loss: 0.02278377115726471
  batch 51 loss: 0.02372099459171295
  batch 52 loss: 0.022925246506929398
  batch 53 loss: 0.024860123172402382
  batch 54 loss: 0.023496504873037338
  batch 55 loss: 0.021649952977895737
  batch 56 loss: 0.025154758244752884
  batch 57 loss: 0.02653104066848755
  batch 58 loss: 0.022095095366239548
  batch 59 loss: 0.02766408398747444
  batch 60 loss: 0.023971961811184883
  batch 61 loss: 0.025308212265372276
  batch 62 loss: 0.025888614356517792
  batch 63 loss: 0.024492722004652023
  batch 64 loss: 0.025545962154865265
  batch 65 loss: 0.02950873039662838
  batch 66 loss: 0.02437850460410118
  batch 67 loss: 0.022973477840423584
  batch 68 loss: 0.02645738795399666
  batch 69 loss: 0.02515169233083725
  batch 70 loss: 0.02403148077428341
  batch 71 loss: 0.024014443159103394
  batch 72 loss: 0.02829613909125328
  batch 73 loss: 0.02560645155608654
  batch 74 loss: 0.02606772445142269
  batch 75 loss: 0.025671929121017456
  batch 76 loss: 0.02217966876924038
  batch 77 loss: 0.021624522283673286
  batch 78 loss: 0.02627062425017357
  batch 79 loss: 0.026496196165680885
  batch 80 loss: 0.024805599823594093
  batch 81 loss: 0.025805987417697906
  batch 82 loss: 0.022894510999321938
  batch 83 loss: 0.024328231811523438
  batch 84 loss: 0.025706665590405464
  batch 85 loss: 0.026052728295326233
  batch 86 loss: 0.0240134596824646
  batch 87 loss: 0.025383025407791138
  batch 88 loss: 0.024313177913427353
  batch 89 loss: 0.025222565978765488
  batch 90 loss: 0.02483171597123146
  batch 91 loss: 0.025112967938184738
  batch 92 loss: 0.025227200239896774
  batch 93 loss: 0.023591849952936172
  batch 94 loss: 0.02412606030702591
  batch 95 loss: 0.02453211322426796
  batch 96 loss: 0.022773677483201027
  batch 97 loss: 0.02435532584786415
  batch 98 loss: 0.026599789038300514
  batch 99 loss: 0.026651058346033096
  batch 100 loss: 0.022466935217380524
  batch 101 loss: 0.024508778005838394
  batch 102 loss: 0.025536730885505676
  batch 103 loss: 0.023744549602270126
  batch 104 loss: 0.025617850944399834
  batch 105 loss: 0.025232812389731407
  batch 106 loss: 0.02502548322081566
  batch 107 loss: 0.024370018392801285
  batch 108 loss: 0.024593515321612358
  batch 109 loss: 0.02856791764497757
  batch 110 loss: 0.024751219898462296
  batch 111 loss: 0.0221322700381279
  batch 112 loss: 0.029424238950014114
  batch 113 loss: 0.024840597063302994
  batch 114 loss: 0.024929199367761612
  batch 115 loss: 0.022040871903300285
  batch 116 loss: 0.025784866884350777
  batch 117 loss: 0.024765703827142715
  batch 118 loss: 0.02458527684211731
  batch 119 loss: 0.024385936558246613
  batch 120 loss: 0.025596996769309044
  batch 121 loss: 0.026345178484916687
  batch 122 loss: 0.023286733776330948
  batch 123 loss: 0.026113003492355347
  batch 124 loss: 0.0249467846006155
  batch 125 loss: 0.02151845395565033
  batch 126 loss: 0.023899033665657043
  batch 127 loss: 0.02599305287003517
  batch 128 loss: 0.023125596344470978
  batch 129 loss: 0.023843731731176376
  batch 130 loss: 0.02585463970899582
  batch 131 loss: 0.024883506819605827
  batch 132 loss: 0.024862168356776237
  batch 133 loss: 0.023726532235741615
  batch 134 loss: 0.023123128339648247
  batch 135 loss: 0.024810370057821274
  batch 136 loss: 0.024424728006124496
  batch 137 loss: 0.030748743563890457
  batch 138 loss: 0.025907523930072784
  batch 139 loss: 0.02315514162182808
  batch 140 loss: 0.02365778014063835
  batch 141 loss: 0.025951579213142395
  batch 142 loss: 0.022545799612998962
  batch 143 loss: 0.025441471487283707
  batch 144 loss: 0.02782556228339672
  batch 145 loss: 0.023998990654945374
  batch 146 loss: 0.02271530032157898
  batch 147 loss: 0.026004858314990997
  batch 148 loss: 0.020266547799110413
  batch 149 loss: 0.023899123072624207
  batch 150 loss: 0.027091670781373978
*********************matrice de confusion***************train******epoch  0
              precision    recall  f1-score   support
         0.0       0.48      0.39      0.43       153
         1.0       0.47      0.56      0.51       147
    accuracy                           0.47       300
   macro avg       0.47      0.48      0.47       300
weighted avg       0.47      0.47      0.47       300
*********************matrice de confusion***************validation******epoch  0
              precision    recall  f1-score   support
         0.0       0.57      0.42      0.48        50
         1.0       0.55      0.69      0.61        51
    accuracy                           0.55       101
   macro avg       0.56      0.55      0.55       101
weighted avg       0.56      0.55      0.55       101
LOSS train 5.5257581174373624e-05 valid 0.024496635422110558
acc train 0.47333333333333333 valid 0.5544554455445545
precision train 0.47333333333333333 valid 0.5544554455445545
EPOCH 2:
  batch 1 loss: 0.025730304419994354
  batch 1 loss: 5.146060883998871e-05
  batch 2 loss: 0.02575180120766163
  batch 3 loss: 0.025595439597964287
  batch 4 loss: 0.026275668293237686
  batch 5 loss: 0.024088330566883087
  batch 6 loss: 0.022517628967761993
  batch 7 loss: 0.025488700717687607
  batch 8 loss: 0.024988802149891853
  batch 9 loss: 0.022595122456550598
  batch 10 loss: 0.02542203478515148
  batch 11 loss: 0.026094013825058937
  batch 12 loss: 0.02546902932226658
  batch 13 loss: 0.025368787348270416
  batch 14 loss: 0.024360384792089462
  batch 15 loss: 0.025677170604467392
  batch 16 loss: 0.023728232830762863
  batch 17 loss: 0.024333108216524124
  batch 18 loss: 0.021520111709833145
  batch 19 loss: 0.024888349696993828
  batch 20 loss: 0.02248312160372734
  batch 21 loss: 0.023122325539588928
  batch 22 loss: 0.025055991485714912
  batch 23 loss: 0.023044999688863754
  batch 24 loss: 0.02576642483472824
  batch 25 loss: 0.024730755016207695
  batch 26 loss: 0.02379610762000084
  batch 27 loss: 0.02474597468972206
  batch 28 loss: 0.022367436438798904
  batch 29 loss: 0.02041989006102085
  batch 30 loss: 0.02316254749894142
  batch 31 loss: 0.02293447032570839
  batch 32 loss: 0.023755114525556564
  batch 33 loss: 0.022703854367136955
  batch 34 loss: 0.024302620440721512
  batch 35 loss: 0.023700956255197525
  batch 36 loss: 0.02421429008245468
  batch 37 loss: 0.023450184613466263
  batch 38 loss: 0.02442176640033722
  batch 39 loss: 0.024149712175130844
  batch 40 loss: 0.02567540854215622
  batch 41 loss: 0.02213170938193798
  batch 42 loss: 0.02329842746257782
  batch 43 loss: 0.023480694741010666
  batch 44 loss: 0.026852622628211975
  batch 45 loss: 0.023539647459983826
  batch 46 loss: 0.02417021617293358
  batch 47 loss: 0.02534317597746849
  batch 48 loss: 0.022848788648843765
  batch 49 loss: 0.023707859218120575
  batch 50 loss: 0.0221547968685627
  batch 51 loss: 0.024966074153780937
  batch 52 loss: 0.022952791303396225
  batch 53 loss: 0.024115853011608124
  batch 54 loss: 0.022753626108169556
  batch 55 loss: 0.020882468670606613
  batch 56 loss: 0.025295011699199677
  batch 57 loss: 0.025218170136213303
  batch 58 loss: 0.02170327678322792
  batch 59 loss: 0.02749350294470787
  batch 60 loss: 0.023312343284487724
  batch 61 loss: 0.026218784973025322
  batch 62 loss: 0.02472669444978237
  batch 63 loss: 0.024706779047846794
  batch 64 loss: 0.025026055052876472
  batch 65 loss: 0.02836412750184536
  batch 66 loss: 0.02396668680012226
  batch 67 loss: 0.02470497600734234
  batch 68 loss: 0.026573341339826584
  batch 69 loss: 0.02483905479311943
  batch 70 loss: 0.024651937186717987
  batch 71 loss: 0.02369886450469494
  batch 72 loss: 0.026588937267661095
  batch 73 loss: 0.02551613189280033
  batch 74 loss: 0.023152384907007217
  batch 75 loss: 0.023241445422172546
  batch 76 loss: 0.022470857948064804
  batch 77 loss: 0.022630348801612854
  batch 78 loss: 0.024481244385242462
  batch 79 loss: 0.026272732764482498
  batch 80 loss: 0.026249568909406662
  batch 81 loss: 0.02562166377902031
  batch 82 loss: 0.02301376312971115
  batch 83 loss: 0.02479524351656437
  batch 84 loss: 0.024091321974992752
  batch 85 loss: 0.024755585938692093
  batch 86 loss: 0.022600898519158363
  batch 87 loss: 0.024566587060689926
  batch 88 loss: 0.02432740479707718
  batch 89 loss: 0.023487353697419167
  batch 90 loss: 0.024877682328224182
  batch 91 loss: 0.023691311478614807
  batch 92 loss: 0.025539718568325043
  batch 93 loss: 0.022038836032152176
  batch 94 loss: 0.024258166551589966
  batch 95 loss: 0.026237070560455322
  batch 96 loss: 0.02373114973306656
  batch 97 loss: 0.023351125419139862
  batch 98 loss: 0.028447583317756653
  batch 99 loss: 0.02472979575395584
  batch 100 loss: 0.023239556699991226
  batch 101 loss: 0.023782622069120407
  batch 102 loss: 0.022317901253700256
  batch 103 loss: 0.025081489235162735
  batch 104 loss: 0.02426057681441307
  batch 105 loss: 0.026302052661776543
  batch 106 loss: 0.02469271793961525
  batch 107 loss: 0.024181274697184563
  batch 108 loss: 0.024781610816717148
  batch 109 loss: 0.02612869255244732
  batch 110 loss: 0.02419595792889595
  batch 111 loss: 0.022778412327170372
  batch 112 loss: 0.025174546986818314
  batch 113 loss: 0.024424973875284195
  batch 114 loss: 0.02564051002264023
  batch 115 loss: 0.02304920181632042
  batch 116 loss: 0.02478523924946785
  batch 117 loss: 0.024778032675385475
  batch 118 loss: 0.023750679567456245
  batch 119 loss: 0.020481403917074203
  batch 120 loss: 0.02797916904091835
  batch 121 loss: 0.02329157665371895
  batch 122 loss: 0.02593407966196537
  batch 123 loss: 0.023290075361728668
  batch 124 loss: 0.02189764939248562
  batch 125 loss: 0.021196862682700157
  batch 126 loss: 0.02514011040329933
  batch 127 loss: 0.024207109585404396
  batch 128 loss: 0.02149735577404499
  batch 129 loss: 0.02278594672679901
  batch 130 loss: 0.026409819722175598
  batch 131 loss: 0.02321755886077881
  batch 132 loss: 0.024321299046278
